{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Quartz/aistudio-workshops/blob/master/jj-searching-documents-with-use.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "co7MV6sX7Xto"
   },
   "source": [
    "# Document Search with Sentence Encoder\n",
    "\n",
    "Quartz Nicar etc.\n",
    "\n",
    "https://github.com/Quartz/aistudio-searching-data-dumps-with-use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOTzp8O36CyQ"
   },
   "source": [
    "# Getting Started\n",
    "\n",
    "This section sets up the environment for access to the Multilingual Universal Sentence Encoder Module and also prepares a set of English sentences and their translations. In the following sections, the multilingual module will be used to compute similarity *across languages*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "63Pd3nJnTl-i"
   },
   "source": [
    "**IMPORTANT**Note: Pleaseelect \"**Python 3**\" _and_ \"**GPU**\" in the ***Runtime->Change Runtime type*** dropdown menu above _before_ running this notebook for faster execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "lVjNK8shFKOC",
    "outputId": "b749a92c-edc1-4b81-8f07-889c5dd52db4"
   },
   "outputs": [],
   "source": [
    "#@title Setup Environment\n",
    "#latest Tensorflow that supports sentencepiece is 1.14\n",
    "!pip uninstall --quiet --yes tensorflow\n",
    "!pip install --quiet tensorflow-gpu==1.14\n",
    "!pip install --quiet tensorflow==1.14\n",
    "!pip install --quiet tensorflow-hub\n",
    "!pip install --quiet bokeh\n",
    "!pip install --quiet tf-sentencepiece\n",
    "!pip install --quiet annoy\n",
    "!pip install --quiet tqdm\n",
    "!pip install --quiet w3lib\n",
    "!pip install --quiet syntok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "MSeY-MUQo2Ha",
    "outputId": "3fdb984f-487d-40ee-b8f2-4439d91f5423"
   },
   "outputs": [],
   "source": [
    "#@title Setup common imports and functions\n",
    "%tensorflow_version 1.x\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tf_sentencepiece  # Not used directly but needed to import TF ops.\n",
    "import sklearn.metrics.pairwise\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gk2IRjZFGDsK"
   },
   "source": [
    "This is additional boilerplate code where we import the pre-trained ML model we will use to encode text throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mkmF3w8WGLcM",
    "outputId": "a1ba0b06-1c62-49ef-cd7c-0b30926bf1ef"
   },
   "outputs": [],
   "source": [
    "# this version of the Universal Sentence Encoder only \"speaks\" English\n",
    "# but there's another version you can switch in that supports 16 different languages!\n",
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder/2'\n",
    "\n",
    "# boilerplate, getting started with Tensorflow.\n",
    "# (how to use Tensorflow is way outside the scope of this class)\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "  text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "  multiling_embed = hub.Module(module_url)\n",
    "  embedded_text = multiling_embed(text_input)\n",
    "  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "g.finalize()\n",
    "\n",
    "session = tf.Session(graph=g)\n",
    "session.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "_XjNsxF2b7ZU",
    "outputId": "95fb0bf3-4f0c-484c-ad80-dc09e7a09a01"
   },
   "outputs": [],
   "source": [
    "# let's get our data!\n",
    "# it's a JSONL file, which is a file with one page, as its own JSON document, per line.\n",
    "!wget -nc -O nyc_docs.jsonl https://raw.githubusercontent.com/Quartz/aistudio-searching-data-dumps-with-use/master/nyc_docs.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVmKpe5hnsxx"
   },
   "source": [
    "## Chopping each page into a list of sentences\n",
    "\n",
    "We have to do this because pages and paragraphs often cover multiple topics, which might confuse the model. And, Universal Sentence Encoder is built to encode sentences... and so it ignores anything after the 128th word in its input.\n",
    "\n",
    "The code below cuts the text into sentences, but groups any two consecutive sentences under 15 words long together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LEh77ZORnrrr",
    "outputId": "2274733d-2da5-4a9d-f598-12321cf6febe"
   },
   "outputs": [],
   "source": [
    "# takes about 10 seconds\n",
    "\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from functools import reduce\n",
    "from w3lib.html import remove_tags\n",
    "\n",
    "import syntok.segmenter as segmenter\n",
    "\n",
    "total_docs = 4251 # get this with `wc` (only used for progress bar)\n",
    "\n",
    "total_short_paragraphs = 0\n",
    "MAX_SENT_LEN = 100\n",
    "\n",
    "def sentenceify(text):\n",
    "    return [sl for l in [[''.join([t.spacing + t.value for t in s]) for s in p if len(s) < MAX_SENT_LEN] for p in segmenter.analyze(text)] for sl in l if any(map(lambda x: x.isalpha(), sl))]\n",
    "\n",
    "\n",
    "def clean_html(html):\n",
    "    if \"<\" in html and \">\" in html:\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "            plist = soup.find('plist')\n",
    "            if plist:\n",
    "                plist.decompose() # remove plists because ugh\n",
    "            text = soup.getText()\n",
    "        except:\n",
    "            text = remove_tags(html)\n",
    "        return '. '.join(text.split(\"\\r\\n\\r\\n\\r\\n\"))\n",
    "    else:\n",
    "        return '. '.join(html.split(\"\\r\\n\\r\\n\\r\\n\"))\n",
    "\n",
    "# if this sentence is short, then group it with other short sentences (so you get groups of continuous short sentences, broken up by one-element groups of longer sentences)\n",
    "def short_sentence_grouper_bean_factory(target_sentence_length): # in chars\n",
    "    def group_short_sentences(list_of_lists_of_sentences, next_sentence):\n",
    "        if not list_of_lists_of_sentences:\n",
    "            return [[next_sentence]]\n",
    "        if len(next_sentence) < target_sentence_length:\n",
    "           list_of_lists_of_sentences[-1].append(next_sentence)\n",
    "        else:\n",
    "            list_of_lists_of_sentences.append([next_sentence])\n",
    "            list_of_lists_of_sentences.append([])\n",
    "        return list_of_lists_of_sentences\n",
    "    return group_short_sentences\n",
    "\n",
    "\n",
    "def overlap(document_tokens, target_length):\n",
    "    \"\"\" pseudo-paginate a document by creating lists of tokens of length `target-length` that overlap at 50%\n",
    "    return a list of `target_length`-length lists of tokens, overlapping by 50% representing all the tokens in the document \n",
    "    \"\"\"\n",
    "\n",
    "    overlapped = []\n",
    "    cursor = 0\n",
    "    while len(' '.join(document_tokens[cursor:]).split()) >= target_length:\n",
    "      overlapped.append(document_tokens[cursor:cursor+target_length])\n",
    "      cursor += target_length // 2\n",
    "    return overlapped\n",
    "\n",
    "\n",
    "def sentences_to_short_paragraphs(group_of_sentences, target_length, min_shingle_length=10):\n",
    "    \"\"\" outputting overlapping groups of shorter sentences \n",
    "    \n",
    "        group_of_sentences = list of strings, where each string is a sentences\n",
    "        target_length = max length IN WORDS of output sentennces\n",
    "        min_shingle_length = don't have sentences that differ just in the inclusion of a sentence of this size\n",
    "    \"\"\"\n",
    "    if len(group_of_sentences) == 1:\n",
    "        return [' '.join(group_of_sentences[0].split())]\n",
    "    sentences_as_words = [sent.split() for sent in group_of_sentences]\n",
    "    sentences_as_words = [sentence for sentence in sentences_as_words if [len(word) for word in sentence].count(1) < (len(sentence) * 0.5) ]\n",
    "    paragraphs = []\n",
    "    for i, sentence in enumerate(sentences_as_words[:-1]):\n",
    "        if i > 0 and len(sentence) < min_shingle_length  and len(sentences_as_words[i-1]) < min_shingle_length and i % 2 == 0:\n",
    "            continue # skip really short sentences if the previous one is also really short (but not so often that we lose anything )\n",
    "        buff = list(sentence) # just making a copy.\n",
    "        for subsequent_sentence in sentences_as_words[i+1:]:\n",
    "            if len(buff) + len(subsequent_sentence) <= target_length:\n",
    "                buff += subsequent_sentence\n",
    "            else:\n",
    "                break\n",
    "        paragraphs.append(buff)\n",
    "    return [' '.join(graf) for graf in paragraphs]\n",
    "\n",
    "\n",
    "def to_short_paragraphs(text, paragraph_len=15, min_sentence_len=8): # paragraph_len in words, min_sentence_len in chars\n",
    "    sentences = sentenceify( clean_html(text) )\n",
    "    grouped_sentences = reduce(short_sentence_grouper_bean_factory(150) , sentences, [])\n",
    "    return [sl for l in [sentences_to_short_paragraphs(group, paragraph_len) for group in grouped_sentences if len(group) >= 2 or (len(group) > 0 and len(group[0]) > min_sentence_len)] for sl in l if sl]\n",
    "\n",
    "paragraph_target_length = 15\n",
    "\n",
    "with open(f\"nyc_docs-sentences{paragraph_target_length}.json\", 'w') as writer: \n",
    "    with open('nyc_docs.jsonl', 'r') as reader:\n",
    "        for i, line_json in tqdm(enumerate(reader), total=total_docs):\n",
    "            line = json.loads(line_json)\n",
    "            text = line[\"_source\"][\"content\"][:1000000]\n",
    "            for j, page in enumerate(to_short_paragraphs(text, paragraph_target_length)):\n",
    "                total_short_paragraphs += 1\n",
    "                writer.write(json.dumps({\n",
    "                    \"text\": page, \n",
    "                    \"_id\": line[\"_id\"], \n",
    "                    \"chonk\": j,\n",
    "                    # \"routing\": line.get(\"_routing\", None),\n",
    "                    # \"path\": line[\"_source\"][\"path\"]\n",
    "                    }) + \"\\n\")\n",
    "print(f\"total paragraphs: {total_short_paragraphs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxAFAJI9xsAU"
   },
   "source": [
    "# Creating a Multilingual Semantic-Similarity Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m3DIT9uT7Z34"
   },
   "source": [
    "## Using a pre-trained model to transform sentences into vectors\n",
    "\n",
    "We compute embeddings in _batches_ so that they fit in the GPU's RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yRoRT5qCEIYy",
    "outputId": "bc50cb4a-dd07-4db6-ab69-2ee76b4aa320"
   },
   "outputs": [],
   "source": [
    "# Takes about 12 seconds\n",
    "vector_index_chunk = AnnoyIndex(512, 'angular')  # Length of item vector that will be indexed\n",
    "\n",
    "batch_size = 256\n",
    "docs = {}\n",
    "\n",
    "doc_counter = 0\n",
    "with tqdm(total=37281) as pbar:\n",
    "  for j, batch in enumerate(pd.read_json('nyc_docs-sentences15.json', lines=True, chunksize=batch_size)):\n",
    "    batch_vecs = session.run(embedded_text, feed_dict={text_input: batch[\"text\"]})\n",
    "    # sentences.extend(batch[\"text\"])\n",
    "    pbar.update(len(batch))\n",
    "    doc_idxs = list(range(doc_counter, doc_counter + batch_size))\n",
    "    for vec, page_num, doc in zip(batch_vecs, doc_idxs, batch.iterrows()):\n",
    "      vector_index_chunk.add_item(page_num, vec)\n",
    "      docs[page_num] = doc[1][\"_id\"]\n",
    "    doc_counter += batch_size\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oeBqoE8e-scg"
   },
   "source": [
    "## Building an index of semantic vectors\n",
    "\n",
    "We use the [Annoy](https://github.com/spotify/annoy) library---to efficiently look up results from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SmoB9I9Pe4IT",
    "outputId": "1c64387c-e0d9-40dc-9bdf-12471e651366"
   },
   "outputs": [],
   "source": [
    "vector_index_chunk.build(10) # 10 trees\n",
    "vector_index_chunk.save('nyc_docs_annoy_small.bin') # you could save this and skip the step above, if you'd like\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jRVX_CFopDyy"
   },
   "source": [
    "What's indexed in Annoy is a meaningless set of 512 numbers for each sentence. Computers can sort of understand this, but humans can't. So we load up into memory the list of all the sentences, so we can print those as the result.\n",
    "\n",
    "This demo uses a fairly small (5mb) set of documents. If you were using this in \"real life\" you'd probably want to use a database to hold onto these -- they'd be too big to hold in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XxzNAzI6mwtH"
   },
   "outputs": [],
   "source": [
    "doc_texts = pd.read_json('nyc_docs-sentences15.json', lines=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kg9cw0S2_ntQ"
   },
   "source": [
    "## Verify that the semantic-similarity search engine works\n",
    "\n",
    "Let's search for some stuff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dxu66S8wJIG9"
   },
   "source": [
    "*   Try a few different sample sentences\n",
    "*   Try changing the number of returned results (they are returned in order of similarity)\n",
    "\n",
    "Once you've tried it out a bit, click the menu button to the left, and click Form -> Show Code to see what this is doing under the hood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "_EFSd65B_mq8",
    "outputId": "d7c43ff7-27bf-44fc-f682-36a442b65bf2"
   },
   "outputs": [],
   "source": [
    "sample_query = 'Subway times are getting worse'  #@param [\"Global warming\", \"Researchers made a surprising new discovery last week.\", \"The stock market fell four points.\", \"Lawmakers will vote on the proposal tomorrow.\"] {allow-input: true}\n",
    "num_results = 10  #@param {type:\"slider\", min:0, max:50, step:1}\n",
    "\n",
    "query_embedding = session.run(embedded_text, feed_dict={text_input: [sample_query]})[0]\n",
    "\n",
    "search_results = vector_index_chunk.get_nns_by_vector(query_embedding, n=num_results)\n",
    "\n",
    "print('sentences similar to: \"{}\"\\n'.format(sample_query))\n",
    "# search_results\n",
    "\n",
    "for idx, result_idx in enumerate(search_results):\n",
    "  page_num = docs[result_idx]\n",
    "  text = doc_texts[(doc_texts[\"_id\"] == page_num)][\"text\"].iloc[0]\n",
    "  # sentence_results[idx] = (page_num, text)\n",
    "  print(f\"{page_num}: {text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "311mS63qpp84"
   },
   "source": [
    "**Copyright 2019 The TensorFlow Hub Authors and Quartz.**\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JMyTNwSJGGWg"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 The TensorFlow Hub Authors and Quartz All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Mayoral USE searcher for NICAR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
