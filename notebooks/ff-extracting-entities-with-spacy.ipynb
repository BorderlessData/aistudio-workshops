{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extratction with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We're going to look for all the people mentioned in a pile of documents.\n",
    "\n",
    "### Entites\n",
    "\n",
    "\"Entities\" in documents are, generally, names -- names of people, places, and things such as companies. Finding out which entities are mentioned in a trove of documents can be pretty helpful, especially when you don't previously _know_ someone or some place is included the document.\n",
    "\n",
    "There are services online that do this kind of extraction, including [DocumentCloud](https://www.documentcloud.org/) ([see how here](https://www.documentcloud.org/faq#faq-analyzing-1)), [Amazon Comprehend](https://aws.amazon.com/comprehend/features/) and [Google Natural Language](https://cloud.google.com/natural-language/).\n",
    "\n",
    "### Using spaCy\n",
    "\n",
    "We're going to do our entity extraction right here in our notebook using a pre-trained natural language model called [spaCy](https://spacy.io/). Specifically, we're using the spaCy [large English language model](https://spacy.io/models/en#en_core_web_lg) trained on the [OntoNotes dataset](https://catalog.ldc.upenn.edu/LDC2013T19) -- a trove of \"telephone conversations, newswire, newsgroups, broadcast news, broadcast conversation, weblogs\" that includes nearly 1.5 million English words.  \n",
    "\n",
    "The spaCy project has a lot of great language features. We'll be looking at the [named entities feature](https://spacy.io/usage/linguistic-features#named-entities). Note also that there are [models for several languages](https://spacy.io/models) being developed in spaCy.\n",
    "\n",
    "\n",
    "## The Plan\n",
    "\n",
    "- We'll download the spaCy software and the large English language model.\n",
    "- We'll also download a (smallish) pile of emails released in a court case.\n",
    "- We'll learn how to use spaCy functions to extract entities\n",
    "- We'll use the spaCy functions to scan all the pages of the emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "This notebook was written by John Keefe [Quartz](https://qz.com) at Quartz and includes document-processing code written included in [a blog post](https://qz.ai/discovering-interesting-documents-in-the-mauritius-leaks/) and a [Jupyter notebook](https://github.com/Quartz/aistudio-doc2vec-for-investigative-journalism/blob/master/Doc2vec%20for%20Investigative%20Journalism.ipynb) by Jeremy B. Merrill at Quartz, who used it to help find documents inside a document dump known as the [Mauritius Leaks](https://qz.com/1670632/how-quartz-used-ai-to-help-reporters-search-the-mauritius-leaks/).  \n",
    "\n",
    "-- John Keefe, [Quartz](https://qz.com), October 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For those using Google Colaboratory ...\n",
    "\n",
    "Be aware that Google Colab instances are ephemeral -- they vanish *Poof* when you close them, or after a period of sitting idle (currently 90 minutes), or if you use one for more than 12 hours.\n",
    "\n",
    "If you're using Google Colaboratory, be sure to set your runtime to \"GPU\" which speeds up your notebook for machine learning:\n",
    "\n",
    "![change runtime](https://qz-aistudio-public.s3.amazonaws.com/workshops/notebook_images/change_runtime_2.jpg)\n",
    "![pick gpu](https://qz-aistudio-public.s3.amazonaws.com/workshops/notebook_images/pick_gpu_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everybody do this ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone needs to run the next cell, which initializes the Python libraries we'll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## *EVERYBODY* SHOULD RUN THIS CELL\n",
    "## This can take up to 3 minutes ... but that's normal\n",
    "%cat /usr/local/cuda/version.txt\n",
    "\n",
    "!pip install -U spacy[cuda100] --quiet\n",
    "!python -m spacy download en_core_web_lg\n",
    "!pip install PyPDF2 --quiet\n",
    "\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "import en_core_web_lg\n",
    "import PyPDF2\n",
    "import json\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we're going to look at some emails from the office of New York City mayor Bill de Blasio that were released under the Freedom of Information Law. \n",
    "\n",
    "The emails were part of the [\"Agent of the City\" hubbub](https://www.ny1.com/nyc/all-boroughs/news/2018/05/24/agents-of-the-city-emails-released), in which 4,000 city emails were released. You can download the [original file here](https://a860-openrecords.nyc.gov/response/120252?token=c784372fd140497081b4bfcff9f0e3a0) -- though we'll be using a file containing just [the first 100 pages](https://qz-aistudio-public.s3.amazonaws.com/workshops/2018.05.24_BerlinRosen_Responsive_Records_100pgs.pdf) for this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to download the data we'll use for this exercise\n",
    "!wget -N https://qz-aistudio-public.s3.amazonaws.com/workshops/deblasio_emails_data.zip --quiet\n",
    "!unzip -q deblasio_emails_data.zip\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's look at what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%ls data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the entity extraction feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we load the model into the notebook\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now let's give it a try\n",
    "doc = nlp(u\"San Francisco considers banning sidewalk delivery robots\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_, spacy.explain(entity.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's [a whole list of entities spaCy can detect](https://spacy.io/api/annotation#named-entities)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_story = \"\"\"\n",
    "John drove his Volkswagen Golf north on Interstate 35 to Duluth, Minnesota,\n",
    "where he stopped at the Aerial Lift Bridge and looked out over\n",
    "Lake Superior. \n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(my_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load the emails into a \"jsonl\" file\n",
    "\n",
    "JSONL is a file format that stores data in a JSON file, with each record living on its own line in the file.\n",
    "\n",
    "This next block reads the PDF file and turns it into a JSONL file, which is much easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the PDF file into a new file called 'nyc_docs.jsonl'\n",
    "jsonl_file = \"nyc_docs.jsonl\"\n",
    "if not exists(jsonl_file):\n",
    "    pdf_file = open('data/2018.05.24_BerlinRosen_Responsive_Records_100pgs.pdf', 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    with open(jsonl_file, 'w') as f:\n",
    "        for page_num in range(read_pdf.getNumPages()):\n",
    "            page = read_pdf.getPage(page_num)\n",
    "            page_content = page.extractText().encode('utf-8').decode(\"utf-8\") \n",
    "            f.write(json.dumps({\"_source\": {\"content\": page_content}, \"_id\": f\"p{page_num+1}\"}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's take a look at the first few lines of the file\n",
    "!head nyc_docs.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line in the JSON file now represents a single page in the original document. So now we'll step through each line (aka page) and grab all the entities in the text. Then we'll print out all the entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and listing the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(jsonl_file, 'r') as f:        # open the jsonl file\n",
    "    for line in f:                      # loop through each line ...\n",
    "        line = json.loads(line)            # read the line \n",
    "        text = line[\"_source\"][\"content\"]  # grab the text of the email\n",
    "        page_number = line[\"_id\"]          # grab the page number we're on\n",
    "        doc = nlp(text)                    # load the text into the nlp model\n",
    "        for ent in doc.ents:               # loop through each entity in the text...\n",
    "            if (ent.label_ == \"PERSON\"):      # if the entity is a person's name ...\n",
    "                print(page_number, ent.text)  # print the page number and the name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really we want a list of _names_ not pages, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_names = {}\n",
    "\n",
    "with open(jsonl_file, 'r') as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        text = line[\"_source\"][\"content\"]\n",
    "        page_number = line[\"_id\"]\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # loop through the entities in the page\n",
    "        for ent in doc.ents:\n",
    "            \n",
    "            # is the entity is a person ...\n",
    "            if (ent.label_ == \"PERSON\"):\n",
    "                \n",
    "                # check if we already have this entity\n",
    "                if ent.text in list_of_names:\n",
    "                    \n",
    "                    # add this page to the entity's list of pages\n",
    "                    list_of_names[ent.text] += \" \" + page_number\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    # otheriwise start a list of pages\n",
    "                    list_of_names[ent.text] = page_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pages in sorted(list_of_names.items()):\n",
    "    print(name, \"(\" + pages + \")\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once you know a name is _there_ then you can search for it [in the original document]((https://qz-aistudio-public.s3.amazonaws.com/workshops/2018.05.24_BerlinRosen_Responsive_Records_100pgs.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
