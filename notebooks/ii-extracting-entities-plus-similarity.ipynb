{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94gdnTZx3rpX"
   },
   "source": [
    "# Entity Extratction with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oMdrdIFE3rpZ"
   },
   "source": [
    "## Overview\n",
    "\n",
    "We're going to look for all the people mentioned in a pile of documents.\n",
    "\n",
    "### Entites\n",
    "\n",
    "\"Entities\" in documents are, generally, names -- names of people, places, and things such as companies. Finding out which entities are mentioned in a trove of documents can be pretty helpful, especially when you don't previously _know_ someone or some place is included the document.\n",
    "\n",
    "There are services online that do this kind of extraction, including [DocumentCloud](https://www.documentcloud.org/) ([see how here](https://www.documentcloud.org/faq#faq-analyzing-1)), [Amazon Comprehend](https://aws.amazon.com/comprehend/features/) and [Google Natural Language](https://cloud.google.com/natural-language/).\n",
    "\n",
    "### Using spaCy\n",
    "\n",
    "We're going to do our entity extraction right here in our notebook using a pre-trained natural language model called [spaCy](https://spacy.io/). Specifically, we're using the spaCy [large English language model](https://spacy.io/models/en#en_core_web_lg) trained on the [OntoNotes dataset](https://catalog.ldc.upenn.edu/LDC2013T19) -- a trove of \"telephone conversations, newswire, newsgroups, broadcast news, broadcast conversation, weblogs\" that includes nearly 1.5 million English words.  \n",
    "\n",
    "The spaCy project has a lot of great language features. We'll be looking at the [named entities feature](https://spacy.io/usage/linguistic-features#named-entities). Note also that there are [models for several languages](https://spacy.io/models) being developed in spaCy.\n",
    "\n",
    "\n",
    "## The Plan\n",
    "\n",
    "- We'll download the spaCy software and the large English language model.\n",
    "- We'll also download a (smallish) pile of emails released in a court case.\n",
    "- We'll learn how to use spaCy functions to extract entities\n",
    "- We'll use the spaCy functions to scan all the pages of the emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iyic6J1H3rpa"
   },
   "source": [
    "## Credits\n",
    "\n",
    "This notebook was written by John Keefe [Quartz](https://qz.com) at Quartz and includes document-processing code written included in [a blog post](https://qz.ai/discovering-interesting-documents-in-the-mauritius-leaks/) and a [Jupyter notebook](https://github.com/Quartz/aistudio-doc2vec-for-investigative-journalism/blob/master/Doc2vec%20for%20Investigative%20Journalism.ipynb) by Jeremy B. Merrill at Quartz, who used it to help find documents inside a document dump known as the [Mauritius Leaks](https://qz.com/1670632/how-quartz-used-ai-to-help-reporters-search-the-mauritius-leaks/).  \n",
    "\n",
    "-- John Keefe, [Quartz](https://qz.com), October 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZY9wZpxT3rpa"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFQXziPj3rpb"
   },
   "source": [
    "### For those using Google Colaboratory ...\n",
    "\n",
    "Be aware that Google Colab instances are ephemeral -- they vanish *Poof* when you close them, or after a period of sitting idle (currently 90 minutes), or if you use one for more than 12 hours.\n",
    "\n",
    "Note that there's a part of this notebook that doesn't seem to work with a GPU (It's the vector part below) so we'll stay on the CPU for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpmTeT2D3rpd"
   },
   "source": [
    "### Everybody do this ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R-SVR0P03rpe"
   },
   "source": [
    "Everyone needs to run the next cell, which initializes the Python libraries we'll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "id": "Efrdg2kQ3rpf",
    "outputId": "bbd14618-1680-475b-916e-b711bcc93ac9"
   },
   "outputs": [],
   "source": [
    "## *EVERYBODY* SHOULD RUN THIS CELL\n",
    "## Running this can take 3-5 minutes ... but that's normal\n",
    "## When it's done, you should see a line in green: \n",
    "## \"âœ” Download and installation successful\"\n",
    "\n",
    "!pip install -U spacy --quiet\n",
    "!python -m spacy download en_core_web_lg\n",
    "!pip install PyPDF2 --quiet\n",
    "\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "import PyPDF2\n",
    "import json\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezEMpvHJ3rpk"
   },
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BW5XuS5B3rpm"
   },
   "source": [
    "In this tutorial, we're going to look at some emails from the office of New York City mayor Bill de Blasio that were released under the Freedom of Information Law. \n",
    "\n",
    "The emails were part of the [\"Agent of the City\" hubbub](https://www.ny1.com/nyc/all-boroughs/news/2018/05/24/agents-of-the-city-emails-released), in which 4,000 city emails were released. You can download the [original file here](https://a860-openrecords.nyc.gov/response/120252?token=c784372fd140497081b4bfcff9f0e3a0) -- though we'll be using a file containing just [the first 100 pages](https://qz-aistudio-public.s3.amazonaws.com/workshops/2018.05.24_BerlinRosen_Responsive_Records_100pgs.pdf) for this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pPycS4yI3rpn",
    "outputId": "80fa58f1-6280-45b7-9793-e5eb1591a505"
   },
   "outputs": [],
   "source": [
    "# Run this cell to download the data we'll use for this exercise\n",
    "!wget -N https://qz-aistudio-public.s3.amazonaws.com/workshops/deblasio_emails_data.zip --quiet\n",
    "!unzip -q deblasio_emails_data.zip\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "ibllkB0E3rpq"
   },
   "source": [
    "Let's look at what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "OwTwZclU3rpr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%ls data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x8gveu1h3rpu"
   },
   "source": [
    "## Trying spaCy's entity extraction feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "y-_IKlRm3rpv"
   },
   "outputs": [],
   "source": [
    "# First we load the model into the notebook\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QPQcAmcf3rpz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now let's give it a try\n",
    "doc = nlp(u\"San Francisco considers banning sidewalk delivery robots\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QdzIa7TW3rp2"
   },
   "outputs": [],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_, spacy.explain(entity.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1He7j8-X3rp5"
   },
   "source": [
    "There's [a whole list of entities spaCy can detect](https://spacy.io/api/annotation#named-entities)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fRyH0kJ93rp6"
   },
   "outputs": [],
   "source": [
    "my_story = \"\"\"\n",
    "John drove his Volkswagen Golf north on Interstate 35 to Duluth, Minnesota,\n",
    "where he stopped at the Aerial Lift Bridge and looked out over\n",
    "Lake Superior. \n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(my_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "PkDd5uRn3rp-"
   },
   "source": [
    "## Load the emails into a \"jsonl\" file\n",
    "\n",
    "JSONL is a file format that stores data in a JSON file, with each record living on its own line in the file.\n",
    "\n",
    "This next block reads the PDF file and turns it into a JSONL file, which is much easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "I-VcNyXh3rp_"
   },
   "outputs": [],
   "source": [
    "# read the PDF file into a new file called 'nyc_docs.jsonl'\n",
    "jsonl_file = \"nyc_docs.jsonl\"\n",
    "if not exists(jsonl_file):\n",
    "    pdf_file = open('data/2018.05.24_BerlinRosen_Responsive_Records_100pgs.pdf', 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    with open(jsonl_file, 'w') as f:\n",
    "        for page_num in range(read_pdf.getNumPages()):\n",
    "            page = read_pdf.getPage(page_num)\n",
    "            page_content = page.extractText().encode('utf-8').decode(\"utf-8\") \n",
    "            f.write(json.dumps({\"_source\": {\"content\": page_content}, \"_id\": f\"p{page_num+1}\"}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "E5gRR5bn3rqC"
   },
   "outputs": [],
   "source": [
    "# let's take a look at the first few lines of the file\n",
    "!head nyc_docs.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tE85H30i3rqF"
   },
   "source": [
    "Each line in the JSON file now represents a single page in the original document. So now we'll step through each line (aka page) and grab all the entities in the text. Then we'll print out all the entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8mtlX2a3rqG"
   },
   "source": [
    "## Finding and listing the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mqW7OlaA3rqH"
   },
   "outputs": [],
   "source": [
    "with open(jsonl_file, 'r') as f:        # open the jsonl file\n",
    "    for line in f:                      # loop through each line ...\n",
    "        line = json.loads(line)            # read the line \n",
    "        text = line[\"_source\"][\"content\"]  # grab the text of the email\n",
    "        page_number = line[\"_id\"]          # grab the page number we're on\n",
    "        doc = nlp(text)                    # load the text into the nlp model\n",
    "        for ent in doc.ents:               # loop through each entity in the text...\n",
    "            if (ent.label_ == \"PERSON\"):      # if the entity is a person's name ...\n",
    "                print(page_number, ent.text)  # print the page number and the name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yljmasbP3rqK"
   },
   "source": [
    "Really we want a list of _names_ not pages, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fx0ornSi3rqL"
   },
   "outputs": [],
   "source": [
    "list_of_names = {}\n",
    "\n",
    "with open(jsonl_file, 'r') as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        text = line[\"_source\"][\"content\"]\n",
    "        page_number = line[\"_id\"]\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # loop through the entities in the page\n",
    "        for ent in doc.ents:\n",
    "            \n",
    "            # is the entity is a person ...\n",
    "            if (ent.label_ == \"PERSON\"):\n",
    "                \n",
    "                # check if we already have this entity\n",
    "                if ent.text in list_of_names:\n",
    "                    \n",
    "                    # add this page to the entity's list of pages\n",
    "                    list_of_names[ent.text] += \" \" + page_number\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    # otheriwise start a list of pages\n",
    "                    list_of_names[ent.text] = page_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AtjOnAvd3rqP"
   },
   "outputs": [],
   "source": [
    "list_of_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "i7s4-BBV3rqT"
   },
   "outputs": [],
   "source": [
    "for name, pages in sorted(list_of_names.items()):\n",
    "    print(name, \"(\" + pages + \")\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "g_-sizHy3rqX"
   },
   "source": [
    "Once you know a name is _there_ then you can search for it [in the original document](https://qz-aistudio-public.s3.amazonaws.com/workshops/2018.05.24_BerlinRosen_Responsive_Records_100pgs.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "QwfSW0zl3rqX"
   },
   "source": [
    "# Detecting Document Similarity with spaCy\n",
    "\n",
    "Now we'll use our pre-trained spaCy model to detect the _similarity_ each of the has with each of the other pages. This will allow us to find pages that are similarly-worded .\n",
    "\n",
    "## The Data\n",
    "\n",
    "All of the data and the installation steps above still apply.\n",
    "\n",
    "## Credits\n",
    "\n",
    "I found [this Medium post super helpful](https://medium.com/better-programming/the-beginners-guide-to-similarity-matching-using-spacy-782fc2922f7c), and used some of the code described there in the `process_text` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6PrtHUp3rqY"
   },
   "source": [
    "## Code\n",
    "\n",
    "### First, let's clean up our data\n",
    "\n",
    "We're going to remove \"stop\" words, such as _a_ and _the_. You can see all the stop words included in spaCy here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll remove all the punctuation and pronouns. Plus some garbage tokens that came along with the emails we have. Everything gets cleaned up with this reusable function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WeoUM5ce3rqZ"
   },
   "outputs": [],
   "source": [
    "# This function accepts text and returns cleaned-up text\n",
    "\n",
    "def process_text(text):\n",
    "    doc = nlp(text.lower()) # set the spaCy \"doc\" to the lower-case version of the text\n",
    "    \n",
    "    result = []  # we'll store our good tokens here\n",
    "    \n",
    "    for token in doc:   # loop through the tokens in the text\n",
    "        \n",
    "        if token.text in nlp.Defaults.stop_words: # skip (continue) if its a stop word\n",
    "            continue\n",
    "        if token.is_punct:  # skip (continue) if the token is a stop word\n",
    "            continue\n",
    "        if token.lemma_ == '-PRON-': # skip (continue) if its a pronoun word\n",
    "            continue\n",
    "        \n",
    "        # the next two are meant to skip lots of the stray characters\n",
    "        # in this particular data dump\n",
    "            \n",
    "        if \"$\" in token.text: # skip if token is or has a $\n",
    "            continue\n",
    "        if \"#\" in token.text: # skip if token is or has a #\n",
    "            continue\n",
    "            \n",
    "        result.append(token.lemma_) # if we got here, add token to the list\n",
    "        \n",
    "    return \" \".join(result)  # join all the remaining tokens with a space between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "6jPHEwtTBwBd",
    "outputId": "291af082-5114-4e14-eba8-a6c31a2944c3"
   },
   "outputs": [],
   "source": [
    "# This is my original text from earlier\n",
    "my_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Zk9t55NaB3rN",
    "outputId": "250f9c3e-9f8f-439e-c5ff-317585c59e05"
   },
   "outputs": [],
   "source": [
    "# Here's what it looks like if I clean it\n",
    "my_story_cleaned = process_text(my_story)\n",
    "my_story_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "emOx1dRj5YIg"
   },
   "outputs": [],
   "source": [
    "# Here's what it's vector looks like! (it's 300 values)\n",
    "doc = nlp(my_story_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the vector\n",
    "\n",
    "Almost every token in the text will have an associated _vector_ of 300 values -- which is, in a way, its position in 300-dimentional space(!) For a series of words, or an entire page, spaCy averages all the token-vectors into one 300-value vector. [More about vectors in the spaCy documentation](https://spacy.io/usage/vectors-similarity).\n",
    "\n",
    "Here's my little story, as a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xQpM8nU_5eVA",
    "outputId": "a1d4c6f3-897b-4fea-cffa-382edfb9e7ae"
   },
   "outputs": [],
   "source": [
    "print(doc.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ndVnRiEU5g9-"
   },
   "source": [
    "Now we'll loop through all of the lines in our file of emails, and for each one we'll clean the text and check the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tMZfgkTL3rqc"
   },
   "outputs": [],
   "source": [
    "list_of_vectors = []\n",
    "list_of_text = []\n",
    "\n",
    "with open(jsonl_file, 'r') as f:        # open the jsonl file\n",
    "    for line in f:                      # loop through each line ...\n",
    "        line = json.loads(line)            # read the line \n",
    "        text = line[\"_source\"][\"content\"]  # grab the text of the email\n",
    "        cleaned_text = process_text(text)  # clean up the text\n",
    "        doc = nlp(cleaned_text)            # load the text into the nlp model\n",
    "        list_of_vectors.append(doc.vector) # add the vector to the list of vectors\n",
    "        list_of_text.append(line)          # add the page's info to a list, too\n",
    "        print(cleaned_text)                # optionally printing each block of text\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4XEbCvdKHH46"
   },
   "outputs": [],
   "source": [
    "# we need some extra libraries we'll load here:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "BGGJ3MaOHSho",
    "outputId": "5825f629-7e44-4978-81ce-bdc98ad0292f"
   },
   "outputs": [],
   "source": [
    "list_of_text[0]  # let's look at the first (0th) item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "grwlfWEOJXl4",
    "outputId": "1bf7eea1-44ea-44c8-c461-bee10120078f"
   },
   "outputs": [],
   "source": [
    "list_of_text[96] # and here's page 97, the 96th item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y5sippVjMOTh",
    "outputId": "68d3d61d-9464-4253-d01e-ae86e5f65ea5"
   },
   "outputs": [],
   "source": [
    "list_of_vectors[96].shape  # This vector is 300 rows by one column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "We're going to use \"cosine similarity\" to compare this one document to every other document in the set. Here are the [details on how to use the cosine_similarity function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html). There's [more about cosine similarity here](https://www.machinelearningplus.com/nlp/cosine-similarity/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "x7Nr5UXKKT7D",
    "outputId": "03533562-af84-4d08-cd98-c8d50caed189"
   },
   "outputs": [],
   "source": [
    "single_item = list_of_vectors[96].reshape(1,300) # reshape the vector to 1 row of 300 columns\n",
    "\n",
    "similarities = cosine_similarity(single_item, list_of_vectors, dense_output=False)\n",
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in one step we sort those similarities lowest to highest (highest being the most similar) and print out the indexes of the best matches ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1Z9nogI0MaKU",
    "outputId": "618acf68-35e4-42b4-ee12-6c69b8999020"
   },
   "outputs": [],
   "source": [
    "# note: need to use the 0th row here, even tho we only have one row\n",
    "related_docs_indices = similarities[0].argsort()[:-5:-1]\n",
    "print (related_docs_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c8Dq3AYHMsaG",
    "outputId": "a7aa16b4-b87a-4bfa-ced4-bab1d6f5e096"
   },
   "outputs": [],
   "source": [
    "# and here we print out the actual similarity values, higher = more similar\n",
    "print (similarities[0, related_docs_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nH7-2mrCM9lF"
   },
   "outputs": [],
   "source": [
    "list_of_text[99]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ii-extracting-entities-plus-similarity-spacy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
